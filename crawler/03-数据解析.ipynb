{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "聚焦爬虫:爬取页面中指定的页面内容。\n",
    "    - 编码流程：\n",
    "        - 指定url\n",
    "        - 发起请求\n",
    "        - 获取响应数据\n",
    "        - 数据解析\n",
    "        - 持久化存储\n",
    "\n",
    "数据解析分类：\n",
    "    - 正则\n",
    "    - bs4\n",
    "    - xpath（***）\n",
    "\n",
    "数据解析原理概述：\n",
    "    - 解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储\n",
    "    - 1.进行指定标签的定位\n",
    "    - 2.标签或者标签对应的属性中存储的数据值进行提取（解析）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#如何爬取图片数据\n",
    "url = 'https://avatars.mds.yandex.net/i?id=8437e262024b4e10bd9f92d52253abf7-5357304-images-taas-consumers&n=27&h=384&w=480'\n",
    "#content返回的是二进制形式的图片数据\n",
    "# text（字符串） content（二进制）json() (对象)\n",
    "img_data = requests.get(url=url).content\n",
    "with open('./qiutu.jpg','wb') as fp:\n",
    "    fp.write(img_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "正则解析\n",
    "核心：re模块 正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['This', 'is', 'a', 'sample', 'text', 'with', 'some', 'numbers', '12345', 'and', 'symbols', 'like', 'and']\n",
      "Numbers: ['12345']\n",
      "Non-word characters: [' ', ' ', ' ', ' ', ' ', ' ', ' ', ': ', ' ', ' ', ' ', ' # ', ' $.']\n",
      "Spaces: [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 示例字符串\n",
    "text = \"This is a sample text with some numbers: 12345 and symbols like # and $.\"\n",
    "\n",
    "# 匹配所有单词\n",
    "words = re.findall(r'\\b(\\w+)\\b', text)\n",
    "print(\"Words:\", words)\n",
    "\n",
    "# 匹配所有数字\n",
    "numbers = re.findall(r'\\d+', text)\n",
    "print(\"Numbers:\", numbers)\n",
    "\n",
    "# 匹配所有非单词字符\n",
    "non_word_chars = re.findall(r'\\W+', text)\n",
    "print(\"Non-word characters:\", non_word_chars)\n",
    "\n",
    "# 匹配所有空白字符\n",
    "spaces = re.findall(r'\\s+', text)\n",
    "print(\"Spaces:\", spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.jpg 下载成功！\n",
      "1.jpg 下载成功！\n",
      "2.jpg 下载成功！\n",
      "3.jpg 下载成功！\n",
      "4.jpg 下载成功！\n",
      "5.jpg 下载成功！\n",
      "6.jpg 下载成功！\n",
      "7.jpg 下载成功！\n",
      "8.jpg 下载成功！\n",
      "9.jpg 下载成功！\n",
      "10.jpg 下载成功！\n",
      "11.jpg 下载成功！\n",
      "12.jpg 下载成功！\n",
      "13.jpg 下载成功！\n",
      "14.jpg 下载成功！\n",
      "15.jpg 下载成功！\n",
      "16.jpg 下载成功！\n",
      "17.jpg 下载成功！\n",
      "18.jpg 下载成功！\n",
      "19.jpg 下载成功！\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "# 爬取yandex网站图片首页所展现的图片\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\",\n",
    "}\n",
    "url = \"https://yandex.com/images/?utm_source=yandex&utm_medium=com&utm_campaign=morda\"\n",
    "\n",
    "response = requests.get(url=url, headers=headers)\n",
    "data = response.text\n",
    "# with open(\"yandex.html\", \"w\", encoding=\"utf-8\") as fp:\n",
    "#     fp.write(data)\n",
    "\n",
    "ex = r'<div class=\"PopularRequestList-Item PopularRequestList-Item_pos_(\\d+).*?src=\"//(.*?)\".*?</div>'\n",
    "res_list = re.findall(ex, data, re.S)\n",
    "\n",
    "if not os.path.exists(\"yandex\"):\n",
    "    os.mkdir(\"yandex\")\n",
    "for res in res_list:\n",
    "    img_url = \"http://\" + res[1]\n",
    "    img_data = requests.get(url=img_url, headers=headers).content\n",
    "    img_name = res[0] + \".jpg\"\n",
    "    img_path = \"yandex/\" + img_name\n",
    "    with open(img_path, \"wb\") as fp:\n",
    "        fp.write(img_data)\n",
    "    print(img_name, \"下载成功！\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bs4进行数据解析\n",
    "    - 数据解析的原理：\n",
    "        - 1.标签定位\n",
    "        - 2.提取标签、标签属性中存储的数据值\n",
    "    - bs4数据解析的原理：\n",
    "        - 1.实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中\n",
    "        - 2.通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取\n",
    "    - 环境安装：\n",
    "        - pip install bs4\n",
    "        - pip install lxml\n",
    "    - 如何实例化BeautifulSoup对象：\n",
    "        - from bs4 import BeautifulSoup\n",
    "        - 对象的实例化：\n",
    "            - 1.将本地的html文档中的数据加载到该对象中\n",
    "                    fp = open('./test.html','r',encoding='utf-8')\n",
    "                    soup = BeautifulSoup(fp,'lxml')\n",
    "            - 2.将互联网上获取的页面源码加载到该对象中\n",
    "                    page_text = response.text\n",
    "                    soup = BeatifulSoup(page_text,'lxml')\n",
    "        - 提供的用于数据解析的方法和属性：\n",
    "            - soup.tagName:返回的是文档中第一次出现的tagName对应的标签\n",
    "            - soup.find():\n",
    "                - find('tagName'):等同于soup.div\n",
    "                - 属性定位：\n",
    "                    -soup.find('div',class_/id/attr='song')\n",
    "            - soup.find_all('tagName'):返回符合要求的所有标签（列表）\n",
    "        - select：\n",
    "            - select('某种选择器（id，class，标签...选择器）'),返回的是一个列表。\n",
    "            - 层级选择器：\n",
    "                - soup.select('.tang > ul > li > a')：>表示的是一个层级\n",
    "                - oup.select('.tang > ul a')：空格表示的多个层级\n",
    "        - 获取标签之间的文本数据：\n",
    "            - soup.a.text/string/get_text()\n",
    "            - text/get_text():可以获取某一个标签中所有的文本内容\n",
    "            - string：只可以获取该标签下面直系的文本内容\n",
    "        - 获取标签中属性值：\n",
    "            - soup.a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "if __name__ == \"__main__\":\n",
    "    #将本地的html文档中的数据加载到该对象中\n",
    "    fp = open('./test.html','r',encoding='utf-8')\n",
    "    soup = BeautifulSoup(fp,'lxml')\n",
    "    # print(soup)\n",
    "    # print(soup.a) # soup.tagName 返回的是html中第一次出现的tagName标签\n",
    "    # print(soup.div) \n",
    "    # print(soup.find('div'))  # find('tagName'):等同于soup.div\n",
    "    # print(soup.findAll('p')) \n",
    "    # print(soup.find_all('a',href='http://www.dudu.com'))\n",
    "    # print(type(soup.select('.tang')), soup.select('.tang'))\n",
    "    # print(soup.select('.tang > ul a')[0]['href'])\n",
    "    # print(soup.select('.tang > ul > li > a')[0]['title'])\n",
    "    print(soup.find_all('div')[1].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一回·宴桃园豪杰三结义  斩黄巾英雄首立功 爬取成功！！！\n",
      "第二回·张翼德怒鞭督邮    何国舅谋诛宦竖 爬取成功！！！\n",
      "第三回·议温明董卓叱丁原  馈金珠李肃说吕布 爬取成功！！！\n",
      "第四回·废汉帝陈留践位    谋董贼孟德献刀 爬取成功！！！\n",
      "第五回·发矫诏诸镇应曹公  破关兵三英战吕布 爬取成功！！！\n",
      "第六回·焚金阙董卓行凶    匿玉玺孙坚背约 爬取成功！！！\n",
      "第七回·袁绍磐河战公孙    孙坚跨江击刘表 爬取成功！！！\n",
      "第八回·王司徒巧使连环计  董太师大闹凤仪亭 爬取成功！！！\n",
      "第九回·除暴凶吕布助司徒  犯长安李傕听贾诩 爬取成功！！！\n",
      "第一十回·勤王室马腾举义    报父仇曹操兴师 爬取成功！！！\n",
      "第十一回·刘皇叔北海救孔融  吕温侯濮阳破曹操 爬取成功！！！\n",
      "第十二回·陶恭祖三让徐州    曹孟德大战吕布 爬取成功！！！\n",
      "第十三回·李傕郭汜大交兵  杨奉董承双救驾 爬取成功！！！\n",
      "第十四回·曹孟德移驾幸许都  吕奉先乘夜袭徐郡 爬取成功！！！\n",
      "第十五回·太史慈酣斗小霸王  孙伯符大战严白虎 爬取成功！！！\n",
      "第十六回·吕奉先射戟辕门    曹孟德败师淯水 爬取成功！！！\n",
      "第十七回·袁公路大起七军    曹孟德会合三将 爬取成功！！！\n",
      "第十八回·贾文和料敌决胜    夏侯惇拔矢啖睛 爬取成功！！！\n",
      "第十九回·下邳城曹操鏖兵    白门楼吕布殒命 爬取成功！！！\n",
      "第二十回·曹阿瞒许田打围    董国舅内阁受诏 爬取成功！！！\n",
      "第二十一回·曹操煮酒论英雄  关公赚城斩车胄 爬取成功！！！\n",
      "第二十二回·袁曹各起马步三军  关张共擒王刘二将 爬取成功！！！\n",
      "第二十三回·祢正平裸衣骂贼    吉太医下毒遭刑 爬取成功！！！\n",
      "第二十四回·国贼行凶杀贵妃    皇叔败走投袁绍 爬取成功！！！\n",
      "第二十五回·屯土山关公约三事  救白马曹操解重围 爬取成功！！！\n",
      "第二十六回·袁本初败兵折将    关云长挂印封金 爬取成功！！！\n",
      "第二十七回·美髯公千里走单骑  汉寿侯五关斩六将 爬取成功！！！\n",
      "第二十八回·斩蔡阳兄弟释疑    会古城主臣聚义 爬取成功！！！\n",
      "第二十九回·小霸王怒斩于吉    碧眼儿坐领江东 爬取成功！！！\n",
      "第三十回·战官渡本初败绩  劫乌巢孟德烧粮 爬取成功！！！\n",
      "第三十一回·曹操仓亭破本初    玄德荆州依刘表 爬取成功！！！\n",
      "第三十二回·夺冀州袁尚争锋    决漳河许攸献计 爬取成功！！！\n",
      "第三十三回·曹丕乘乱纳甄氏    郭嘉遗计定辽东 爬取成功！！！\n",
      "第三十四回·蔡夫人隔屏听密语  刘皇叔跃马过檀溪 爬取成功！！！\n",
      "第三十五回·玄德南漳逢隐沧    单福新野遇英主 爬取成功！！！\n",
      "第三十六回·玄德用计袭樊城    元直走马荐诸葛 爬取成功！！！\n",
      "第三十七回·司马徽再荐名士    刘玄德三顾草庐 爬取成功！！！\n",
      "第三十八回·定三分隆中决策    战长江孙氏报仇 爬取成功！！！\n",
      "第三十九回·荆州城公子三求计  博望坡军师初用兵 爬取成功！！！\n",
      "第四十回·蔡夫人议献荆州    诸葛亮火烧新野 爬取成功！！！\n",
      "第四十一回·刘玄德携民渡江    赵子龙单骑救主 爬取成功！！！\n",
      "第四十二回·张翼德大闹长坂桥  刘豫州败走汉津口 爬取成功！！！\n",
      "第四十三回·诸葛亮舌战群儒    鲁子敬力排众议 爬取成功！！！\n",
      "第四十四回·孔明用智激周瑜    孙权决计破曹操 爬取成功！！！\n",
      "第四十五回·三江口曹操折兵    群英会蒋干中计 爬取成功！！！\n",
      "第四十六回·用奇谋孔明借箭    献密计黄盖受刑 爬取成功！！！\n",
      "第四十七回·阚泽密献诈降书    庞统巧授连环计 爬取成功！！！\n",
      "第四十八回·宴长江曹操赋诗    锁战船北军用武 爬取成功！！！\n",
      "第四十九回·七星坛诸葛祭风    三江口周瑜纵火 爬取成功！！！\n",
      "第五十回·诸葛亮智算华容    关云长义释曹操 爬取成功！！！\n",
      "第五十一回·曹仁大战东吴兵    孔明一气周公瑾 爬取成功！！！\n",
      "第五十二回·诸葛亮智辞鲁肃    赵子龙计取桂阳 爬取成功！！！\n",
      "第五十三回·关云长义释黄汉升  孙仲谋大战张文远 爬取成功！！！\n",
      "第五十四回·吴国太佛寺看新郎  刘皇叔洞房续佳偶 爬取成功！！！\n",
      "第五十五回·玄德智激孙夫人    孔明二气周公瑾 爬取成功！！！\n",
      "第五十六回·曹操大宴铜雀台    孔明三气周公瑾 爬取成功！！！\n",
      "第五十七回·柴桑口卧龙吊丧    耒阳县凤雏理事 爬取成功！！！\n",
      "第五十八回·马孟起兴兵雪恨    曹阿瞒割须弃袍 爬取成功！！！\n",
      "第五十九回·许诸裸衣斗马超    曹操抹书问韩遂 爬取成功！！！\n",
      "第六十回·张永年反难杨修    庞士元议取西蜀 爬取成功！！！\n",
      "第六十一回·赵云截江夺阿斗    孙权遗书退老瞒 爬取成功！！！\n",
      "第六十二回·取涪关杨高授首    攻雒城黄魏争功 爬取成功！！！\n",
      "第六十三回·诸葛亮痛哭庞统    张翼德义释严颜 爬取成功！！！\n",
      "第六十四回·孔明定计捉张任    杨阜借兵破马超 爬取成功！！！\n",
      "第六十五回·马超大战葭萌关    刘备自领益州牧 爬取成功！！！\n",
      "第六十六回·关云长单刀赴会    伏皇后为国捐生 爬取成功！！！\n",
      "第六十七回·曹操平定汉中地    张辽威震逍遥津 爬取成功！！！\n",
      "第六十八回·甘宁百骑劫魏营    左慈掷杯戏曹操 爬取成功！！！\n",
      "第六十九回·卜周易管辂知机    讨汉贼五臣死节 爬取成功！！！\n",
      "第七十回·猛张飞智取瓦口隘  老黄忠计夺天荡山 爬取成功！！！\n",
      "第七十一回·占对山黄忠逸待劳  据汉水赵云寡胜众 爬取成功！！！\n",
      "第七十二回·诸葛亮智取汉中    曹阿瞒兵退斜谷 爬取成功！！！\n",
      "第七十三回·玄德进位汉中王    云长攻拔襄阳郡 爬取成功！！！\n",
      "第七十四回·庞令明抬榇决死战  关云长放水淹七军 爬取成功！！！\n",
      "第七十五回·关云长刮骨疗毒    吕子明白衣渡江 爬取成功！！！\n",
      "第七十六回·徐公明大战沔水    关云长败走麦城 爬取成功！！！\n",
      "第七十七回·玉泉山关公显圣    洛阳城曹操感神 爬取成功！！！\n",
      "第七十八回·治风疾神医身死    传遗命奸雄数终 爬取成功！！！\n",
      "第七十九回·兄逼弟曹植赋诗    侄陷叔刘封伏法 爬取成功！！！\n",
      "第八十回·曹丕废帝篡炎刘    汉王正位续大统 爬取成功！！！\n",
      "第八十一回·急兄仇张飞遇害    雪弟恨先主兴兵 爬取成功！！！\n",
      "第八十二回·孙权降魏受九锡    先主征吴赏六军 爬取成功！！！\n",
      "第八十三回·战猇亭先主得仇人  守江口书生拜大将 爬取成功！！！\n",
      "第八十四回·陆逊营烧七百里    孔明巧布八阵图 爬取成功！！！\n",
      "第八十五回·刘先主遗诏托孤儿  诸葛亮安居平五路 爬取成功！！！\n",
      "第八十六回·难张温秦宓逞天辩  破曹丕徐盛用火攻 爬取成功！！！\n",
      "第八十七回·征南寇丞相大兴师  抗天兵蛮王初受执 爬取成功！！！\n",
      "第八十八回·渡泸水再缚番王    识诈降三擒孟获 爬取成功！！！\n",
      "第八十九回·武乡侯四番用计    南蛮王五次遭擒 爬取成功！！！\n",
      "第九十回·驱巨善六破蛮兵    烧藤甲七擒孟获 爬取成功！！！\n",
      "第九十一回·祭泸水汉相班师    伐中原武侯上表 爬取成功！！！\n",
      "第九十二回·赵子龙力斩五将    诸葛亮智取三城 爬取成功！！！\n",
      "第九十三回·姜伯约归降孔明    武乡侯骂死王朝 爬取成功！！！\n",
      "第九十四回·诸葛亮乘雪破羌兵  司马懿克日擒孟达 爬取成功！！！\n",
      "第九十五回·马谡拒谏失街亭    武侯弹琴退仲达 爬取成功！！！\n",
      "第九十六回·孔明挥泪斩马谡    周鲂断发赚曹休 爬取成功！！！\n",
      "第九十七回·讨魏国武侯再上表  破曹兵姜维诈献书 爬取成功！！！\n",
      "第九十八回·追汉军王双受诛    袭陈仓武侯取胜 爬取成功！！！\n",
      "第九十九回·诸葛亮大破魏兵    司马懿入寇西蜀 爬取成功！！！\n",
      "第一百回·汉兵劫寨破曹真    武侯斗阵辱仲达 爬取成功！！！\n",
      "第一百十一回·出陇上诸葛妆神    奔剑阁张郃中计 爬取成功！！！\n",
      "第一百十二回·司马懿占北原渭桥  诸葛亮造木牛流马 爬取成功！！！\n",
      "第一百十三回·上方谷司马受困    五丈原诸葛禳星 爬取成功！！！\n",
      "第一百十四回·陨大星汉丞相归天  见木像魏都督丧胆 爬取成功！！！\n",
      "第一百十五回·武侯预伏锦囊计    魏主拆取承露盘 爬取成功！！！\n",
      "第一百十六回·公孙渊兵败死襄平  司马懿诈病赚曹爽 爬取成功！！！\n",
      "第一百十七回·魏主政归司马氏    姜维兵败牛头山 爬取成功！！！\n",
      "第一百十八回·丁奉雪中奋短兵    孙峻席间施密计 爬取成功！！！\n",
      "第一百十九回·困司马汉将奇谋    废曹芳魏家果报 爬取成功！！！\n",
      "第一百一十回·文鸯单骑退雄兵    姜维背水破大敌 爬取成功！！！\n",
      "第一百一十一回·邓士载智败姜伯约  诸葛诞义讨司马昭 爬取成功！！！\n",
      "第一百一十二回·救寿春于诠死节    取长城伯约鏖兵 爬取成功！！！\n",
      "第一百一十三回·丁奉定计斩孙綝    姜维斗阵破邓艾 爬取成功！！！\n",
      "第一百一十四回·曹髦驱车死南阙    姜维弃粮胜魏兵 爬取成功！！！\n",
      "第一百一十五回·诏班师后主信谗    托屯田姜维避祸 爬取成功！！！\n",
      "第一百一十六回·钟会分兵汉中道    武侯显圣定军山 爬取成功！！！\n",
      "第一百一十七回·邓士载偷度阴平    诸葛瞻战死绵竹 爬取成功！！！\n",
      "第一百一十八回·哭祖庙一王死孝    入西川二士争功 爬取成功！！！\n",
      "第一百一十九回·假投降巧计成虚话  再受禅依样画葫芦 爬取成功！！！\n",
      "第一百二十回·荐杜预老将献新谋  降孙皓三分归一统 爬取成功！！！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 爬取三国演义小说所有的章节标题和章节内容http://www.shicimingju.com/book/sanguoyanyi.html\n",
    "#对首页的页面数据进行爬取\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'\n",
    "}\n",
    "url = 'http://www.shicimingju.com/book/sanguoyanyi.html'\n",
    "page_text = requests.get(url=url,headers=headers).content.decode('utf-8')\n",
    "# page_text = requests.get(url=url,headers=headers)\n",
    "# page_text.encoding = 'utf-8'\n",
    "# page_text = page_text.text\n",
    "\n",
    "#在首页中解析出章节的标题和详情页的url\n",
    "soup = BeautifulSoup(page_text,'lxml')\n",
    "#解析章节标题和详情页的url\n",
    "li_list = soup.select('.book-mulu > ul > li')\n",
    "fp = open('./sanguo.txt', 'w', encoding='utf-8')\n",
    "for li in li_list:\n",
    "    title = li.a.string\n",
    "    detail_url = 'http://www.shicimingju.com'+li.a['href']\n",
    "    #对详情页发起请求，解析出章节内容\n",
    "    detail_page_text = requests.get(url=detail_url,headers=headers).content\n",
    "    #解析出详情页中相关的章节内容\n",
    "    detail_soup = BeautifulSoup(detail_page_text,'lxml')\n",
    "    div_tag = detail_soup.find('div',class_='chapter_content')\n",
    "    #解析到了章节的内容\n",
    "    content = div_tag.text\n",
    "    fp.write(title+':'+content+'\\n')\n",
    "    print(title,'爬取成功！！！')\n",
    "    # break\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xpath解析：最常用且最便捷高效的一种解析方式。通用性。\n",
    "\n",
    "    - xpath解析原理：\n",
    "        - 1.实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。\n",
    "        - 2.调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。\n",
    "    - 环境的安装：\n",
    "        - pip install lxml\n",
    "    - 如何实例化一个etree对象:from lxml import etree\n",
    "        - 1.将本地的html文档中的源码数据加载到etree对象中：\n",
    "            etree.parse(filePath)\n",
    "        - 2.可以将从互联网上获取的源码数据加载到该对象中\n",
    "            etree.HTML('page_text')\n",
    "        - xpath('xpath表达式')\n",
    "    - xpath表达式:\n",
    "        - /:表示的是从根节点开始定位。表示的是一个层级。\n",
    "        - //:表示的是多个层级。可以表示从任意位置开始定位。\n",
    "        - 属性定位：//tag[@attrName=\"attrValue\"]\n",
    "        - 索引定位：//div[@class=\"song\"]/p[3] 索引是从1开始的。\n",
    "        - 取文本：\n",
    "            - /text() 获取的是标签中直系的文本内容\n",
    "            - //text() 标签中非直系的文本内容（所有的文本内容）\n",
    "        - 取属性：\n",
    "            /@attrName     ==>img/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.song.com/', '', 'http://www.baidu.com/meinv.jpg']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python \n",
    "# -*- coding:utf-8 -*-\n",
    "from lxml import html\n",
    "etree = html.etree\n",
    "#实例化好了一个etree对象，且将被解析的源码加载到了该对象中\n",
    "tree = etree.parse('test.html')\n",
    "# r = tree.xpath('/html/body/div')\n",
    "# r = tree.xpath('/html//div')\n",
    "# r = tree.xpath('//div')\n",
    "# r = tree.xpath('//div[@class=\"song\"]')\n",
    "# r = tree.xpath('//div[@class=\"tang\"]//li//text()')\n",
    "# r = tree.xpath('//li[7]/i/text()')\n",
    "# r = tree.xpath('//div[@class=\"tang\"]//text()')\n",
    "# r = tree.xpath('//div[@class=\"song\"]//@href')\n",
    "r = tree.xpath('//div[@class=\"song\"]//@href | //@src')\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['急急售，稳定签约！双井站，全凸户型大两居，商品房，随时看', '哇塞！！朝阳园 落地窗 无遮挡 视野开阔 配套齐 精致']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "#需求：爬取58二手房中的房源信息\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "#爬取到页面源码数据\n",
    "url = 'https://bj.58.com/ershoufang/'\n",
    "page_text = requests.get(url=url,headers=headers).content.decode('utf-8')\n",
    "\n",
    "#数据解析\n",
    "tree = etree.HTML(page_text)\n",
    "#存储的就是li标签对象\n",
    "li_list = tree.xpath('//h3/text()')\n",
    "print(li_list[0:2])\n",
    "fp = open('58.txt','w',encoding='utf-8')\n",
    "for li in li_list:\n",
    "    #局部解析\n",
    "    fp.write(li+'\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO-8859-1\n",
      "高清美女.jpg 下载成功！！！\n",
      "刘亦菲.jpg 下载成功！！！\n",
      "张婧仪.jpg 下载成功！！！\n",
      "油伞.jpg 下载成功！！！\n",
      "森系.jpg 下载成功！！！\n",
      "唯美古风美女包上恩8k壁纸.jpg 下载成功！！！\n",
      "古风美女包上恩.jpg 下载成功！！！\n",
      "漂亮古风美女.jpg 下载成功！！！\n",
      "双马尾辫.jpg 下载成功！！！\n",
      "森林.jpg 下载成功！！！\n",
      "田曦薇.jpg 下载成功！！！\n",
      "居家.jpg 下载成功！！！\n",
      "古力娜扎.jpg 下载成功！！！\n",
      "高清美女.jpg 下载成功！！！\n",
      "白色毛衣.jpg 下载成功！！！\n",
      "长头发.jpg 下载成功！！！\n",
      "王楚然.jpg 下载成功！！！\n",
      "陈瑶.jpg 下载成功！！！\n",
      "红色古装新娘.jpg 下载成功！！！\n",
      "长发养眼漂亮美女陈瑶3440x1440带鱼屏壁纸.jpg 下载成功！！！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import os\n",
    "\n",
    "#需求：解析下载图片数据 http://pic.netbian.com/4kmeinv/\n",
    "\n",
    "url = 'http://pic.netbian.com/4kmeinv/'\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
    "}\n",
    "response = requests.get(url=url,headers=headers)\n",
    "print(response.encoding)\n",
    "page_text = response.content\n",
    "\n",
    "#数据解析：src的属性值  alt属性\n",
    "tree = etree.HTML(page_text)\n",
    "li_list = tree.xpath('//div[@class=\"slist\"]/ul/li')\n",
    "\n",
    "#创建一个文件夹\n",
    "if not os.path.exists('./meinv'):\n",
    "    os.mkdir('./meinv')\n",
    "\n",
    "for li in li_list:\n",
    "    img_src = 'http://pic.netbian.com'+li.xpath('./a/img/@src')[0]\n",
    "    img_name = li.xpath('./a/img/@alt')[0].split(' ')[0]+'.jpg'\n",
    "    #通用处理中文乱码的解决方案\n",
    "    # img_name = img_name.encode('iso-8859-1').decode('gbk')\n",
    "    #请求图片进行持久化存储\n",
    "    img_data = requests.get(url=img_src,headers=headers).content\n",
    "    img_path = 'meinv/'+img_name\n",
    "    with open(img_path,'wb') as fp:\n",
    "        fp.write(img_data)\n",
    "        print(img_name,'下载成功！！！')\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '上海', '广州', '深圳', '杭州', '天津', '成都', '南京', '西安', '武汉', '阿坝州', '安康', '阿克苏地区', '阿里地区', '阿拉善盟', '阿勒泰地区', '安庆', '安顺', '鞍山', '克孜勒苏州', '安阳', '蚌埠', '白城', '保定', '北海', '宝鸡', '北京', '毕节', '博州', '白山', '百色', '保山', '白沙', '包头', '保亭', '本溪', '巴彦淖尔', '白银', '巴中', '滨州', '亳州', '长春', '昌都', '常德', '成都', '承德', '赤峰', '昌吉州', '五家渠', '昌江', '澄迈', '重庆', '长沙', '常熟', '楚雄州', '朝阳', '沧州', '长治', '常州', '潮州', '郴州', '池州', '崇左', '滁州', '定安', '丹东', '东方', '东莞', '德宏州', '大理州', '大连', '大庆', '大同', '定西', '大兴安岭地区', '德阳', '东营', '黔南州', '达州', '德州', '儋州', '鄂尔多斯', '恩施州', '鄂州', '防城港', '佛山', '抚顺', '阜新', '阜阳', '富阳', '抚州', '福州', '广安', '贵港', '桂林', '果洛州', '甘南州', '固原', '广元', '贵阳', '甘孜州', '赣州', '广州', '淮安', '海北州', '鹤壁', '淮北', '河池', '海东地区', '邯郸', '哈尔滨', '合肥', '鹤岗', '黄冈', '黑河', '红河州', '怀化', '呼和浩特', '海口', '呼伦贝尔', '葫芦岛', '哈密地区', '海门', '海南州', '淮南', '黄南州', '衡水', '黄山', '黄石', '和田地区', '海西州', '河源', '衡阳', '汉中', '杭州', '菏泽', '贺州', '湖州', '惠州', '吉安', '金昌', '晋城', '景德镇', '金华', '西双版纳州', '九江', '吉林', '即墨', '江门', '荆门', '佳木斯', '济南', '济宁', '胶南', '酒泉', '句容', '湘西州', '金坛', '鸡西', '嘉兴', '江阴', '揭阳', '济源', '嘉峪关', '胶州', '焦作', '锦州', '晋中', '荆州', '库尔勒', '开封', '黔东南州', '克拉玛依', '昆明', '喀什地区', '昆山', '临安', '六安', '来宾', '聊城', '临沧', '娄底', '乐东', '廊坊', '临汾', '临高', '漯河', '丽江', '吕梁', '陇南', '六盘水', '拉萨', '乐山', '丽水', '凉山州', '陵水', '莱芜', '莱西', '临夏州', '溧阳', '辽阳', '辽源', '临沂', '龙岩', '洛阳', '连云港', '莱州', '兰州', '林芝', '柳州', '泸州', '马鞍山', '牡丹江', '茂名', '眉山', '绵阳', '梅州', '宁波', '南昌', '南充', '宁德', '内江', '南京', '怒江州', '南宁', '南平', '那曲地区', '南通', '南阳', '平度', '平顶山', '普洱', '盘锦', '蓬莱', '平凉', '莆田', '萍乡', '濮阳', '攀枝花', '青岛', '琼海', '秦皇岛', '曲靖', '潜江', '齐齐哈尔', '七台河', '黔西南州', '清远', '庆阳', '钦州', '衢州', '泉州', '琼中', '荣成', '日喀则', '乳山', '日照', '韶关', '寿光', '上海', '绥化', '石河子', '石家庄', '商洛', '三明', '三门峡', '山南', '遂宁', '神农架林区', '四平', '商丘', '宿迁', '上饶', '汕头', '汕尾', '绍兴', '三亚', '邵阳', '沈阳', '十堰', '松原', '双鸭山', '深圳', '朔州', '宿州', '随州', '苏州', '石嘴山', '泰安', '塔城地区', '太仓', '铜川', '屯昌', '通化', '天津', '铁岭', '通辽', '铜陵', '吐鲁番地区', '天门', '铜仁地区', '唐山', '天水', '太原', '台州', '泰州', '文昌', '文登', '潍坊', '瓦房店', '威海', '乌海', '芜湖', '武汉', '吴江', '乌兰察布', '乌鲁木齐', '渭南', '万宁', '文山州', '武威', '无锡', '温州', '吴忠', '梧州', '五指山', '西安', '兴安盟', '许昌', '宣城', '襄阳', '孝感', '迪庆州', '辛集', '锡林郭勒盟', '厦门', '西宁', '咸宁', '湘潭', '邢台', '仙桃', '新乡', '咸阳', '新余', '信阳', '忻州', '徐州', '雅安', '延安', '延边州', '宜宾', '盐城', '宜昌', '宜春', '银川', '运城', '伊春', '云浮', '阳江', '营口', '榆林', '玉林', '伊犁哈萨克州', '阳泉', '玉树州', '烟台', '鹰潭', '义乌', '宜兴', '玉溪', '益阳', '岳阳', '扬州', '永州', '淄博', '自贡', '珠海', '湛江', '镇江', '诸暨', '张家港', '张家界', '张家口', '周口', '驻马店', '章丘', '肇庆', '中山', '舟山', '昭通', '中卫', '张掖', '招远', '资阳', '遵义', '枣庄', '漳州', '郑州', '株洲'] 399\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "#项目需求：解析出所有城市名称https://www.aqistudy.cn/historydata/\n",
    "\n",
    "# headers = {\n",
    "#     'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
    "# }\n",
    "# url = 'https://www.aqistudy.cn/historydata/'\n",
    "# page_text = requests.get(url=url,headers=headers).text\n",
    "#\n",
    "# tree = etree.HTML(page_text)\n",
    "# host_li_list = tree.xpath('//div[@class=\"bottom\"]/ul/li')\n",
    "# all_city_names = []\n",
    "# #解析到了热门城市的城市名称\n",
    "# for li in host_li_list:\n",
    "#     hot_city_name = li.xpath('./a/text()')[0]\n",
    "#     all_city_names.append(hot_city_name)\n",
    "#\n",
    "# #解析的是全部城市的名称\n",
    "# city_names_list = tree.xpath('//div[@class=\"bottom\"]/ul/div[2]/li')\n",
    "# for li in city_names_list:\n",
    "#     city_name = li.xpath('./a/text()')[0]\n",
    "#     all_city_names.append(city_name)\n",
    "#\n",
    "# print(all_city_names,len(all_city_names))\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
    "}\n",
    "url = 'https://www.aqistudy.cn/historydata/'\n",
    "page_text = requests.get(url=url, headers=headers).content.decode('utf-8')\n",
    "\n",
    "tree = etree.HTML(page_text)\n",
    "#解析到热门城市和所有城市对应的a标签\n",
    "# //div[@class=\"bottom\"]/ul/li/          热门城市a标签的层级关系\n",
    "# //div[@class=\"bottom\"]/ul/div[2]/li/a  全部城市a标签的层级关系\n",
    "a_list = tree.xpath('//div[@class=\"bottom\"]/ul/li/a | //div[@class=\"bottom\"]/ul/div[2]/li/a')\n",
    "all_city_names = []\n",
    "for a in a_list:\n",
    "    city_name = a.xpath('./text()')[0]\n",
    "    all_city_names.append(city_name)\n",
    "print(all_city_names,len(all_city_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线程-ThreadPoolExecutor-1_0 正在爬取  https://sc.chinaz.com/psd/index.html?business=0\n",
      "线程-ThreadPoolExecutor-1_1 正在爬取  https://sc.chinaz.com/psd/index_2.html?business=0\n",
      "线程-ThreadPoolExecutor-1_1 卡通插画元素节日邀请A5模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 手工艺术展宣传单模板设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 曲棍球活动宣传单PSD模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 志愿者招募A5传单设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 珠宝配饰A5传单模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 唇釉化妆品广告传单设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 宠物店宠物护理折扣传单A5模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 周岁宝宝生日宴会邀请模板A5 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 扁平插画风格房地产海报设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 可爱卡通风格周岁生日海报设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 曲棍球运动传单PS素材 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 人工智能AI机器人海报设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 汉堡美食横幅广告PS素材 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 全民营养周宣传海报PSD模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 游戏活动传单设计源文件 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 世界心脏日公益活动邀请模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 卡通插画风格市场营销传单设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 游戏电竞海报PSD设计源文件 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 房地产建筑卡通插画风格传单设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 蓝色清新花卉派对邀请A5模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 犹太新年庆祝传单设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 1周岁生日邀请A5模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 手工展艺术展活动传单设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 足球运动活动邀请A5模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 全国营养周宣传单A5模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_0 珠宝配饰三折页正反面模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 美味餐饮美食INS模板设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 餐饮美食行业名片PSD模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 夏日主题banner设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 汉堡快餐美食传单海报PSD模板 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 正方形婚礼邀请卡片设计 下载成功!\n",
      "线程-ThreadPoolExecutor-1_1 餐饮美食招贴传单A5模板 下载成功!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from lxml import etree\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# 下载 https://sc.chinaz.com/psd/?business=0 免费设计模版\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"d_template\"):\n",
    "    os.mkdir(\"d_template\")\n",
    "\n",
    "def download_template(src, name):\n",
    "    try:\n",
    "        template = requests.get(url=src, headers=headers).content.decode('utf-8')\n",
    "        template_tree = etree.HTML(template)\n",
    "        download_url = template_tree.xpath('//div[@class=\"c-div clearfix\"]/a/@href')[0]\n",
    "        template = requests.get(url=download_url, headers=headers).content\n",
    "        with open(\"d_template/\"+name+\".zip\", \"wb\") as fp:\n",
    "            fp.write(template)\n",
    "        print(f\"线程-{threading.current_thread().name} {name} 下载成功!\")\n",
    "    except Exception as e:\n",
    "        print(f\"线程-{threading.current_thread().name} {name} 下载失败: {e}\")\n",
    "\n",
    "# 分页\n",
    "def scrape_page(index):\n",
    "    page = \"index\"+(\"_\"+str(index) if index > 1 else \"\")+\".html\"\n",
    "    url = 'https://sc.chinaz.com/psd/'+page+\"?business=0\"\n",
    "    print(f\"线程-{threading.current_thread().name} 正在爬取 \", url)\n",
    "    page_text = requests.get(url=url, headers=headers).content.decode('utf-8')\n",
    "    tree = etree.HTML(page_text)\n",
    "    items = tree.xpath('//div[@class=\"bot-div\"]/a')\n",
    "    for item in items:\n",
    "        name = item.xpath('./text()')[0]\n",
    "        src = \"https:\"+item.xpath('./@href')[0]\n",
    "        download_template(src, name)\n",
    "        # time.sleep(1)  # 请求间隔1秒，避免过于频繁的请求\n",
    "\n",
    "# 使用多线程提高爬取速度\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    executor.map(scrape_page, range(1, 3))\n",
    "\n",
    "print(\"所有模版下载完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
